{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5719aa95-cb38-4085-bf6f-44efe74a046f",
   "metadata": {},
   "source": [
    "## Greenburgh Tax Assessment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71eca6-7ac8-4530-b179-8c81f10ba242",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ef26a-686a-4453-b7d8-0500bc17a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import sqlite3 as lite\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c54f4-1830-438e-98b1-93059643f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tax assessment database and Assessment table.\n",
    "TAX_ASSESS_DB = \"/home/rsm/proj/tax_ass/taxdb/taxrec.db\"\n",
    "ASSESS_TABLE='taxrec'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e234f4-b55a-476a-ad40-c075885a8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadbad0-ebdf-43a1-9db7-8841162a1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data from our database.\n",
    "conn  = lite.connect(TAX_ASSESS_DB)\n",
    "query = f\"SELECT * from {ASSESS_TABLE};\"\n",
    "df    = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f0336-2c43-44b1-8ae3-0a6d92227c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need data sorted by year as we will group by YEAR to get a list of FULL_MKT_VALUE(s)\n",
    "## which we will use to create a return series.\n",
    "df = df.sort_values(by='YEAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626aa81f-309d-475f-a717-19a4a41f6f61",
   "metadata": {},
   "source": [
    "### Exceptions\n",
    "These exception classes will be used to throw errors in the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da4ac7-e8f0-4544-bc6b-cd27cca4a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightArrayMisMatch(Exception):\n",
    "  '''\n",
    "    Array and associated weight array do not have the same length.\n",
    "  '''\n",
    "  def __init__(self, message=\"Weight array and associated values array are not the same length.\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) \n",
    "    \n",
    "class NegativeWeightArrayValue(Exception):\n",
    "  '''\n",
    "    Weight array has at least one negative value..\n",
    "  '''\n",
    "  def __init__(self, message=\"Weight array has at least one negative value.\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) \n",
    "    \n",
    "class WeightSumNotPositive(Exception):\n",
    "  '''\n",
    "    Weight array sum is not positive.\n",
    "  '''\n",
    "  def __init__(self, message=\"Sum of weight array values is not positive.\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) \n",
    "    \n",
    "class NotNumpyArray(Exception):\n",
    "  '''\n",
    "    Array is not a numpy array.\n",
    "  '''\n",
    "  def __init__(self, message=\"Array is not a numpy array.\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) \n",
    "\n",
    "class NotAMatrix(Exception):\n",
    "  '''\n",
    "    Array is not a matrix.\n",
    "  '''\n",
    "  def __init__(self, message=\"Array is not a matrix.\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) \n",
    "    \n",
    "class NotProperQuantile(Exception):\n",
    "  '''\n",
    "    Array is not a numpy array.\n",
    "  '''\n",
    "  def __init__(self, message=\"Array as at least one value not in [0, 1].\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f20c24-0e7c-4fbf-bcee-eddd505431b6",
   "metadata": {},
   "source": [
    "### Functions\n",
    "Functions used in the analysis of the Tax Assessment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd0c56-6cda-4733-aaae-0e64c5e5ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wgt_quantiles(vss, wgts, qss):\n",
    "  '''\n",
    "  Get a numpy array (or scalar if qss is scalar) consisting of an array of quantile weighted <vss> values.\n",
    "    \n",
    "  :param vss  A numpy(np) array of values. \n",
    "  :param wgts A numpy(np) array of weights. (These wegiths need only be non-negative, they need not sum to 1.)\n",
    "  :param qss  A numpy(np) array of values.  (Meant to be quantiles -- numbers in the range [0, 1]); OR, a scalar value.\n",
    "  :return An numpy array (or scalar) consisting of the quantile weighted values of <vss> using weights, <wgts>, for each quantile in <qss>.\n",
    "    :type numpy array of numeric values (or scalar) with the same length as <qss>.\n",
    "  \n",
    "  :packages numpy(np)\n",
    "  \n",
    "  :argument contract \n",
    "    1. vss, wgts are all numpy arrays.\n",
    "    2. qss in [0.0, 1.0]\n",
    "    3. |vss| == |wgts|\n",
    "    4. all(wgts) >= 0\n",
    "    5. sum(wgts) > 0\n",
    "\n",
    "  '''\n",
    "  \n",
    "  ## 1. vss and wgts are numpy arrays?\n",
    "  if type(vss)  != np.ndarray:\n",
    "    raise(NotNumpyArray('wgt_quantiles: <vss>: Not an numpy array.' ))\n",
    "  if type(wgts) != np.ndarray:\n",
    "    raise(NotNumpyArray('wgt_quantiles: <wgts>: Not an numpy array.'))\n",
    "  if type(qss)  != np.ndarray:\n",
    "    raise(NotNumpyArray('wgt_quantiles: <qss>: Not an numpy array.'))\n",
    "    \n",
    "  ## 2. All qss values in [0.0, 1.0]?\n",
    "  if any((qss < 0.0) | (qss > 1.0)):\n",
    "    raise(NotProperQuantile('wgt_quantiles: <qss>: Not a proper quantiles array.'))\n",
    "  \n",
    "  ## 3. The length of vss and wgts is the same?\n",
    "  if np.size(vss) != np.size(wgts):\n",
    "    raise(WeightArrayMisMatch('wgt_quantiles: <vss> and <wgts> do not have the same length.'))\n",
    "\n",
    "  ## 4. all wgts >= 0?\n",
    "  if any(wgts < 0.0):\n",
    "    raise(NegativeWeightArrayValue('wgt_quantiles: <wgts> has one or more negative elements.'))\n",
    "\n",
    "  ## 5. sum(wgts) > 0?\n",
    "  if sum(wgts) <= 0:\n",
    "    raise(WeightSumNotPositive('wgt_quantiles: Sum of <wgts> is not positive.'))\n",
    "    \n",
    "  ## Need to reshape these arrays in order to do broadcasting, so first copy them.\n",
    "  vs  = vss.copy()\n",
    "  qs  = qss.copy()\n",
    "  ws  = wgts.copy()\n",
    "  \n",
    "  ## Sort the vs array and the associated weights.\n",
    "  ## Turn the weights into proper weights and create a cummulative weight array.\n",
    "  idx  = np.argsort(vs)\n",
    "  ovs  = vs[idx]\n",
    "  ows  = ws[idx]\n",
    "  ows  = ows / np.sum(ows) # Normalize the weights.\n",
    "  cws  = np.cumsum(ows)\n",
    "  \n",
    "  N    = np.size(cws)\n",
    "  M    = np.size(qs)\n",
    "  \n",
    "  ## Reshape to broadcast.\n",
    "  cws.shape = (N, 1)\n",
    "  qs.shape  = (1, M)\n",
    "  \n",
    "  ## Use broadcasting to get all comparisons of <cws> with each entry from <qs>.  \n",
    "  ## Form tensor (cws <= qs) * 1 and sandwich index of the value vectors with 0 and 1.\n",
    "  A   = np.concatenate([np.ones(M).reshape(1,M), (cws <= qs) * 1, np.zeros(M).reshape(1,M)], axis=0)\n",
    "  \n",
    "  ## Get the diff -- -1 will indicate where the boundary is where cws > qs.\n",
    "  X   = np.diff(A, axis=0).astype(int)\n",
    "  \n",
    "  ## Get the indices of the boundary.\n",
    "  idx = np.maximum(0, np.where(X == -1)[0] - 1)\n",
    "  \n",
    "  ## Return the weighted quantile value of <vs> against each <qs>.\n",
    "  return(ovs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cc47d5-3fcb-4c77-821e-0ee686d2c5fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wgt_quantiles_tensor(vss, wgts, qss):\n",
    "  '''\n",
    "  Get a numpy array consisting of an array of quantile weighted <vss> values.\n",
    "    \n",
    "  :param vss  A numpy(np) array (D, N)  of values. \n",
    "  :param wgts A numpy(np) array of N weights. (These wegiths need only be non-negative, they need not sum to 1.)\n",
    "  :param qss  A numpy(np) array of M values.  (Meant to be quantiles -- numbers in the range [0, 1]); OR, a scalar value.\n",
    "  :return An numpy array consisting of the quantile weighted values of <vss> using weights, <wgts>, for each quantile in <qss>.\n",
    "    :type (D, M) numpy array of numeric values.\n",
    "  \n",
    "  :packages numpy(np)\n",
    "  \n",
    "  :argument contract \n",
    "    1. vss, wgts, and qss are all numpy arrays.\n",
    "    2. vss is a matrix.\n",
    "    3. qss in [0.0, 1.0]\n",
    "    4. |vss[0]| == |wgts|\n",
    "    5. all(wgts) >= 0\n",
    "    6. sum(wgts) > 0\n",
    "\n",
    "  '''\n",
    "  \n",
    "  ## 1. vss and wgts are numpy arrays?\n",
    "  if type(vss)  != np.ndarray:\n",
    "    raise(NotNumpyArray('wgt_quantiles_tensor: <vss>: Not an numpy array.' ))\n",
    "  if type(wgts) != np.ndarray:\n",
    "    raise(NotNumpyArray('wgt_quantiles_tensor: <wgts>: Not an numpy array.'))\n",
    "  if type(qss) != np.ndarray:\n",
    "    raise(NotNumpyArray('wgt_quantiles_tensor: <qss>: Not an numpy array.'))  \n",
    "   \n",
    "  ## 2. vss is a matrix?\n",
    "  if len(vss.shape) != 2:\n",
    "    raise(NotAMatrix('wgt_quantiles_tensor: <vss>: Not a matrix.'))\n",
    "    \n",
    "  ## 3. All qss values in [0.0, 1.0]?\n",
    "  if any((qss < 0.0) | (qss > 1.0)):\n",
    "    raise(NotProperQuantile('wgt_quantiles_tensor: <qss>: Not a proper quantiles array.'))\n",
    "  \n",
    "  ## 4. The length of the elements of vss and wgts are the same?\n",
    "  if np.size(vss[0]) != np.size(wgts):\n",
    "    raise(WeightArrayMisMatch(\"wgt_quantiles_tensor: <vss> elements don't have the same length as <wgts>.\"))\n",
    "\n",
    "  ## 5. all wgts >= 0?\n",
    "  if any(wgts < 0.0):\n",
    "    raise(NegativeWeightArrayValue('wgt_quantiles_tensor: <wgts> has one or more negative elements.'))\n",
    "\n",
    "  ## 6. sum(wgts) > 0?\n",
    "  if sum(wgts) <= 0:\n",
    "    raise(WeightSumNotPositive('wgt_quantiles_tensor: Sum of <wgts> is not positive.'))\n",
    "    \n",
    "  ## Need to reshape these arrays in order to do broadcasting, so first copy them.\n",
    "  vs  = vss.copy()\n",
    "  qs  = qss.copy()\n",
    "  ws  = wgts.copy()\n",
    "  \n",
    "  ## Normalize the weights.\n",
    "  ws  = ws / np.sum(ws)\n",
    "  \n",
    "  D, N  = vs.shape\n",
    "  M     = qs.size\n",
    "\n",
    "  ## Get the sorted index array for each of the value vectors in vs.\n",
    "  idx = np.argsort(vs, axis=1)\n",
    "  \n",
    "  ## Apply this index back to vs to get sorted values.\n",
    "  ovs = np.take_along_axis(vs, idx, axis=1)\n",
    "  \n",
    "  ## Apply the index to the weights, where, the dimension of ws (and cws) expands to: (D, N).\n",
    "  ows = ws[idx]\n",
    "  cws = np.cumsum(ows, axis=1)\n",
    "\n",
    "  ## Reshape to broadcast.\n",
    "  cws.shape = (D, N, 1)\n",
    "  qs.shape  = (1, 1, M)\n",
    "\n",
    "  ## Use broadcasting to get all comparisons of <cws> with each entry from <qs>. \n",
    "  ## Form tensor (cws <= qs) * 1 and sandwich index of the value vectors with 0 and 1.\n",
    "  A = np.concatenate([np.ones(M*D).reshape(D,1,M), (cws <= qs) * 1, np.zeros(M*D).reshape(D,1,M)], axis=1)\n",
    "  \n",
    "  ## Compute the index difference on the value vectors.\n",
    "  Delta = np.diff(A, axis=1).astype(int)\n",
    "\n",
    "  ## Get the index of the values, this leaves, essentially, a (D, M) matrix. Reshape it as such.\n",
    "  idx = np.maximum(0, np.where(Delta == -1)[1] - 1)\n",
    "  idx = idx.reshape(D, M) \n",
    "  \n",
    "  ## Return the values in the value vectors that correspond to these indices -- the M quantiles for each of the D value vectors.\n",
    "  ## A (D, M) matrix.\n",
    "  return(np.take_along_axis(ovs, idx, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dca884-c164-40be-8918-11970f52e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessment_agr_rets(df, ret_field, wgt_field, filt=True):\n",
    "  '''\n",
    "  Computes the weighted averge returns over the date range of the data frame, <df>.\n",
    "    \n",
    "  :param df: A Pandas DataFrame with required fields.\n",
    "           :type pandas.DataFrame.core\n",
    "  :param ret_field: A field representing an numpy array of returns.\n",
    "           :type str\n",
    "  :param wgt_field: A weight field, used to weight the returns in a given row.\n",
    "           :type str\n",
    "           \n",
    "  :returns An numpy array of aggregated returns.\n",
    "  :rtype numpy(np) array(float)\n",
    "  \n",
    "  :packages numpy(np), pandas\n",
    "  '''\n",
    "  \n",
    "  ## If the filter value is a scalar -- replicate it to the length of the dataframe, <df>.\n",
    "  fl = filt\n",
    "  if np.shape(filt) == ():\n",
    "    fl = np.repeat(filt, df.shape[0])\n",
    "\n",
    "  return(np.average(np.stack(df.loc[fl, ret_field]), weights=df.loc[fl, wgt_field], axis=0)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d13558bc-cfb1-4f08-89df-4c9a00aff804",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function does not currently work. There is a mismatch witht he qgt_quantiles function it calls.\n",
    "def assessment_wgt_quant_rets(df, ret_field, wgt_field, quants, filt=True):\n",
    "  '''\n",
    "    Computes weighted quantiles of returns over the date range of the data frame, <df>.\n",
    "    Here, \n",
    "    - df has length N and contains the field names of the returns and the weights from the variables: ret_field and wgt_field.\n",
    "    - D is the number of returns in the ret_field.\n",
    "    - N is the number of weights and return arrays of size D.\n",
    "    - M is the length of the quants array.\n",
    "    \n",
    "  :param df: A Pandas DataFrame with required fields.\n",
    "            :type DataFrame\n",
    "  :param ret_field: A field representing an numpy array of returns.\n",
    "            :type str\n",
    "  :param wgt_field: A weight field, used to weight the returns in a given row.\n",
    "            :type str\n",
    "  :param quants: A scalar or numpy(np) array of quantiles.\n",
    "           \n",
    "  :returns An numpy array of weighted quantile returns.\n",
    "            :rtype (D, M) numpy array(float)\n",
    "  '''\n",
    "  \n",
    "  ## If the filter value is a scalar -- replicate it to the length of the dataframe, <df>.\n",
    "  fl = filt\n",
    "  \n",
    "  ## Apply the filter to the data set.\n",
    "  if np.shape(filt) == ():\n",
    "    fl = np.repeat(filt, df.shape[0])\n",
    "\n",
    "  ## Make a copy of the data set and retrieve the weights (N) and the (D, N) array of returns.\n",
    "  A = df.loc[fl, :].copy()  \n",
    "  W = A[wgt_field].to_numpy()\n",
    "  A = np.stack(A[ret_field]).T\n",
    "\n",
    "  ## Compute the DxM quantile returns.\n",
    "  return(wgt_quantiles_tensor(A, W, quants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f83d1b3-5061-4c1c-a43c-cda729b2f7d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessment_wgt_median_rets(df, ret_field, wgt_field, filt=True):\n",
    "  '''\n",
    "    Computes weighted median over the return arrays in the field <ret_field> using the weights in field, <wgt_field>.\n",
    "    \n",
    "    - df has length N and contains the field names of the returns and the weights from the variables: ret_field and wgt_field.\n",
    "    - D is the number of returns in the ret_field.\n",
    "    - N is the number of weights and return arrays of size D.\n",
    "\n",
    "  :param df: A Pandas DataFrame with required fields.\n",
    "            :type DataFrame\n",
    "  :param ret_field: A field representing an numpy array (D) of returns.\n",
    "            :type str\n",
    "  :param wgt_field: A weight field, used to weight the returns (N) in a given row.\n",
    "            :type str\n",
    "           \n",
    "  :returns An numpy array of weighted quantile returns of length D.\n",
    "            :rtype numpy array(float)\n",
    "  '''\n",
    "  ## If the filter value is a scalar -- replicate it to the length of the dataframe, <df>.\n",
    "  fl = filt\n",
    "  \n",
    "  ## Apply filter.\n",
    "  if np.shape(filt) == ():\n",
    "    fl = np.repeat(filt, df.shape[0])\n",
    "\n",
    "  A = df.loc[fl, :].copy()\n",
    "  W = A[wgt_field].to_numpy()\n",
    "  A = np.stack(A[ret_field]).T\n",
    "  Z = wgt_quantiles_tensor(A, W, np.array([0.5]))\n",
    "  return(Z[:, 0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd65802-690f-4400-82e9-42b907322a9f",
   "metadata": {},
   "source": [
    "### Data Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f297fba-f92c-47ba-9424-f00d4db42222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7c858d-66ac-4394-a19f-32e7ebb818c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PARCEL_TYPE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006193a7-ce1b-47d1-b3a9-d7310f8336f8",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "We use regular expression matching to determine which data should be filtered out.\n",
    "We create a dictionary below, badLinesDct, that contains the number of bad lines\n",
    "for each field of interest: FULL_MKT_VALUE (market value of parcel), ACCT (parcel account id), LUC (Land Use Code), ACCR (parcel acreage size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9cddc-2701-4e28-b403-602f3989e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "badLinesDct = {}\n",
    "good_mkt_filter = df[\"FULL_MKT_VALUE\"].astype(str).str.match(\"^\\d+$\")\n",
    "badLinesDct['FULL_MKT_VALUE']  = df.loc[~ good_mkt_filter].shape[0]\n",
    "\n",
    "good_acct_filter = df[\"ACCT\"].astype(str).str.match(\"^\\d+$\")\n",
    "badLinesDct['ACCT'] = df.loc[~ good_acct_filter].shape[0]\n",
    "\n",
    "good_luc_filter = df['LUC'].astype(str).str.match(\"^\\d+$\")\n",
    "badLinesDct['LUC'] = df.loc[~ good_luc_filter].shape[0]\n",
    "\n",
    "good_accr_filter = df['ACCR'].astype(str).str.match(\"(^(\\d*)\\.\\d+$)|(^\\d+(\\.\\d*)?$)\")\n",
    "badLinesDct['ACCR'] = df.loc[~ good_accr_filter].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb65df-6f9a-47ee-841f-d0a27ab454c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter out the bad mkt value, luc, and acct data.\n",
    "df_filt = df[(good_mkt_filter & good_luc_filter & good_acct_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec595b6-14c2-41dc-b89b-06b5bf725c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = df_filt.groupby('ACCT')['FULL_MKT_VALUE'].agg(lambda x: x.size).reset_index(name='MKT_COUNT')\n",
    "\n",
    "dd1 = df_filt.groupby('ACCT')['FULL_MKT_VALUE'].agg(lambda x: any(x == 0)).reset_index(name='MKT_ZERO')\n",
    "\n",
    "ddd = dd.merge(dd1, on='ACCT', how='inner')\n",
    "\n",
    "## Now get the accounts that extend over the 11 period that we have data mkt value data for AND which aren't zero.\n",
    "accts = ddd.loc[(ddd.MKT_COUNT == 11) & (~ ddd.MKT_ZERO)].ACCT\n",
    "\n",
    "## Only use these accounts from the filtered data. This is the data set we will use for analysis.\n",
    "df_clean = df_filt.loc[df_filt['ACCT'].isin(accts), :]\n",
    "df_clean.to_csv(\"clean_tax_ass.psv\", sep='^', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495f505-5f17-494c-b6a0-e7d42f0a375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Describe the reduction in data after cleaning.\n",
    "print(f\"Data cleaning reduced the overall data set by {100.0 * np.round( (df.shape[0] - df_clean.shape[0]) / df.shape[0], 2)}%.\")\n",
    "raw_residencial_count = df.loc[df.LUC == 210].shape[0]\n",
    "filtered_residencial_count = df_clean.loc[df_clean.LUC == 210].shape[0]\n",
    "print(f\"Data cleaning reduced the residencial data set by {100.0 * np.round( (raw_residencial_count - filtered_residencial_count) / raw_residencial_count, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80837409-a7c4-41ee-8558-c305785ff04a",
   "metadata": {},
   "source": [
    "### Compute Market Return Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f47e5-aed2-40e7-bd8e-b2fc91154146",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a field, 'mkt_vals' that is an np.array of returns (ordered by YEAR).\n",
    "dd = df_clean.groupby('ACCT').apply(lambda row: np.array(row['FULL_MKT_VALUE'])).reset_index(name='mkt_vals')\n",
    "\n",
    "dd['mkt_rets'] = dd['mkt_vals'].apply(lambda x: np.diff(x)) / dd['mkt_vals'].apply(lambda x: x[:-1])\n",
    "\n",
    "dd['avg_mkt_val'] = dd.apply(lambda row: np.mean(row['mkt_vals']), axis=1)\n",
    "\n",
    "df_rets = df_clean.merge(dd, on='ACCT', how='inner')\n",
    "\n",
    "df_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a1ab4-2b2d-40f4-9dfd-a84c84240e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.value_counts(df_rets.LUC).to_frame(name='LUC_cnt').reset_index()\n",
    "dff['log_LUC'] = np.log10(dff.LUC_cnt)\n",
    "ax = dff.plot.scatter(x = 'LUC', y='log_LUC', xlabel='LUC\\n(Residencial LUC=210)', ylabel='Log10 of LUC Count', title=\"Log10 of LUC counts\")\n",
    "ax.axvline(210, linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754bbb7-4dbb-494e-9657-b0375ddbe501",
   "metadata": {},
   "source": [
    "### Aggregate Market Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1b03c-88b7-4b66-b80a-f15b8e780671",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Empty data frame to store returns.\n",
    "df_results = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf1684-ebdd-4581-99a7-78ffdbdfb3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall Aggregated Market Returns\n",
    "df_results['overall'] = df_rets['mkt_rets'].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c353cea-35db-4613-87cc-4cee74d4834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregated Market Returns for LUC 210 -- Single family residence.\n",
    "df_results['residence'] = df_rets.loc[df_rets['LUC'] == 210, 'mkt_rets'].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd2035-2477-4bba-b036-81313074eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e601f-916b-40f2-9d68-489d71d9824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute overall weighted returns using the average market value as the weight \n",
    "## -- Also recompute but restrict analysis to single family residences -- LUC = 210.\n",
    "df_results['overall_mkt_wgt'] = assessment_agr_rets(df_rets, 'mkt_rets', 'avg_mkt_val')\n",
    "df_results['residence_mkt_wgt'] = assessment_agr_rets(df_rets, 'mkt_rets', 'avg_mkt_val', filt = df_rets['LUC'] == 210)\n",
    "df_results['overall_mkt_wgt_med'] = assessment_wgt_quant_rets(df_rets, 'mkt_rets', 'avg_mkt_val', np.array([0.5]))[:, 0] \n",
    "df_results['residence_mkt_wgt_med'] = assessment_wgt_quant_rets(df_rets, 'mkt_rets', 'avg_mkt_val', np.array([0.5]), filt=df_rets['LUC'] == 210)[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82b8e4-f06a-4578-a2c8-0365fae5e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_results[['residence', 'overall_mkt_wgt', 'overall_mkt_wgt_med', 'residence_mkt_wgt', 'residence_mkt_wgt_med']].plot( \n",
    "                     xlabel=\"Year\"                     , \n",
    "                     ylabel=\"Assessment Change from Previous Year\",\n",
    "                     title=\"Tax Assessment Comparison (Greenburgh)\" ,\n",
    "                     xticks=df_results.index, rot=90    )\n",
    "ax.set_xticklabels(['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022']);\n",
    "ax.legend(['Residential Avg', 'Overall Mkt Wgt Avg', 'Overall Mkt Wgt Med', 'Residential Mkt Wgt Avg', 'Residential Mkt Wgt Med']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afd725-cece-4feb-b1e0-db066385365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall Aggregated Market Returns\n",
    "df_results['overall_cs'] = np.cumprod(1.0 + df_rets['mkt_rets'].agg(np.mean)) - 1.0\n",
    "\n",
    "## Aggregated Market Returns for LUC 210 -- Single family residence.\n",
    "df_results['residence_cs'] = np.cumprod(1.0 + df_rets.loc[df_rets['LUC'] == 210, 'mkt_rets'].agg(np.mean)) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ae313-8df4-4e10-8666-441be31608e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ac287-6fdf-43d4-891f-1898611b5550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Compute overall weighted returns (avg and med) using the average market value as the weight.\n",
    "df_results['overall_mkt_wgt_cs'] = np.cumprod(1.0 + assessment_agr_rets(df_rets, 'mkt_rets', 'avg_mkt_val')) - 1.0\n",
    "df_results['overall_mkt_med_cs'] = np.cumprod(1.0 + assessment_wgt_quant_rets(df_rets, 'mkt_rets', 'avg_mkt_val', np.array([0.5]))[:, 0]) - 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad47533-97ab-420a-afcf-9448c1d6ec21",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute overall weighted returns using the average market value as the weight \n",
    "## -- but restrict analysis to single family residences -- LUC = 210.\n",
    "df_results['residence_mkt_wgt_cs'] = np.cumprod(1.0 + assessment_agr_rets(df_rets, 'mkt_rets', 'avg_mkt_val', filt = df_rets['LUC'] == 210)) - 1.0\n",
    "df_results['residence_mkt_med_cs'] = np.cumprod(1.0 + assessment_wgt_quant_rets(df_rets, 'mkt_rets', 'avg_mkt_val', np.array([0.5]), filt=df_rets['LUC'] == 210)[:, 0]) - 1.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06fa8da-60a0-453a-a979-e5a3fb08e78a",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_results[['residence_cs', 'overall_mkt_wgt_cs', 'overall_mkt_med_cs', 'residence_mkt_wgt_cs', 'residence_mkt_med_cs']].plot( \n",
    "                     xlabel=\"Year\"                     , \n",
    "                     ylabel=\"Assessment Change from 2012\",\n",
    "                     title=\"Tax Assessment Comparison (Greenburgh)\\n(Cumulative Change)\" ,\n",
    "                     xticks=df_results.index, rot=90    )\n",
    "ax.set_xticklabels(['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022']);\n",
    "ax.legend(['Residential Avg', 'Overall Mkt Wgt Avg', 'Overall Mkt Med', 'Residential Mkt Wgt Avg', 'Residential Mkt Med']);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
