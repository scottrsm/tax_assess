{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5719aa95-cb38-4085-bf6f-44efe74a046f",
   "metadata": {},
   "source": [
    "## Greenburgh Tax Assessment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae71eca6-7ac8-4530-b179-8c81f10ba242",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "819ef26a-686a-4453-b7d8-0500bc17a4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import sqlite3 as lite\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('ggplot')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e2c54f4-1830-438e-98b1-93059643f279",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Tax assessment database and Assessment table.\n",
    "TAX_ASSESS_DB = \"/home/rsm/proj/tax_ass/taxdb/taxrec.db\"\n",
    "ASSESS_TABLE='taxrec'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e234f4-b55a-476a-ad40-c075885a8645",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_columns', 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecadbad0-ebdf-43a1-9db7-8841162a1e41",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Read in data from our database.\n",
    "conn  = lite.connect(TAX_ASSESS_DB)\n",
    "query = f\"SELECT * from {ASSESS_TABLE};\"\n",
    "df    = pd.read_sql_query(query, conn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418f0336-2c43-44b1-8ae3-0a6d92227c4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## We need data sorted by year as we will group by YEAR to get a list of FULL_MKT_VALUE(s)\n",
    "## which we will use to create a return series.\n",
    "df = df.sort_values(by='YEAR')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "626aa81f-309d-475f-a717-19a4a41f6f61",
   "metadata": {},
   "source": [
    "### Exceptions\n",
    "These exception classes will be used to throw errors in the functions below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95da4ac7-e8f0-4544-bc6b-cd27cca4a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightArrayMisMatch(Exception):\n",
    "  '''\n",
    "    Array and associated weight array do not have the same length.\n",
    "  '''\n",
    "  def __init__(self, message=\"Weight array and associated values array are not the same length.\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) \n",
    "    \n",
    "class NegativeWeightArrayValue(Exception):\n",
    "  '''\n",
    "    Weight array has at least one negative value..\n",
    "  '''\n",
    "  def __init__(self, message=\"Weight array has at least one negative value.\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) \n",
    "    \n",
    "class WeightSumNotPositive(Exception):\n",
    "  '''\n",
    "    Weight array sum is not positive.\n",
    "  '''\n",
    "  def __init__(self, message=\"Sum of weight array values is not positive.\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) \n",
    "    \n",
    "class NotNumpyArray(Exception):\n",
    "  '''\n",
    "    Array is not a numpy array.\n",
    "  '''\n",
    "  def __init__(self, message=\"Array is not a numpy array.\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) \n",
    "    \n",
    "class NotProperQuantile(Exception):\n",
    "  '''\n",
    "    Array is not a numpy array.\n",
    "  '''\n",
    "  def __init__(self, message=\"Array as at least one value not in [0, 1].\"):\n",
    "    self.message = message\n",
    "    super().__init__(self.message) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56f20c24-0e7c-4fbf-bcee-eddd505431b6",
   "metadata": {},
   "source": [
    "### Functions\n",
    "Functions used in the analysis of the Tax Assessment data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcd0c56-6cda-4733-aaae-0e64c5e5ab3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def wgt_quantiles(vss, wgts, qss):\n",
    "  '''\n",
    "    Get a numpy array (or scalar if qss is scalar) consisting of an array of quantile weighted <vss> values.\n",
    "    \n",
    "  :param vss  A numpy(np) array of values. \n",
    "  :param wgts A numpy(np) array of weights. (Standard usage: Cummulative weights (increasing values in the range [0,1]).\n",
    "  :param qss  A numpy(np) array of values.  (Meant to be quantiles -- numbers in the range [0, 1])\n",
    "              OR, a scalar value.\n",
    "  :return An numpy array (or scalar) consisting of the quantile weighted values of <vss> using weights, <wgts>, for each quantile in <qss>.\n",
    "    :type numpy array of numeric values (or scalar) with the same length as <qss>.\n",
    "  \n",
    "  :packages numpy(np)\n",
    "  \n",
    "  :arg-contract \n",
    "    1. vss, wgts are all numpy arrays.\n",
    "    2. qss in [0.0, 1.0]\n",
    "    3. |vss| == |wgts|\n",
    "    4. all(wgts) >= 0\n",
    "    5. sum(wgts) > 0\n",
    "\n",
    "  '''\n",
    "  \n",
    "  ## Check the argument contract...\n",
    "  scalar_quants = False\n",
    "  if type(vss)  != np.ndarray:\n",
    "    raise(NotNumpyArray('wgt_quantiles: <vss>: Not an numpy array.' ))\n",
    "  if type(wgts) != np.ndarray:\n",
    "    raise(NotNumpyArray('wgt_quantiles: <wgts>: Not an numpy array.'))\n",
    "  if type(qss)  != np.ndarray:\n",
    "    qss = np.array([qss])\n",
    "    scalar_quants = True\n",
    "  if any((qss < 0.0) | (qss > 1.0)):\n",
    "    raise(NotProperQuantile('wgt_quantiles: <qss>: Not a proper quantiles array.'))\n",
    "  if np.size(vss) != np.size(wgts):\n",
    "    raise(WeightArrayMisMatch('wgt_quantiles: <vss> and <wgts> do not have the same length.'))\n",
    "  if any(wgts < 0.0):\n",
    "    raise(NegativeWeightArrayValue('wgt_quantiles: <wgts> has one or more negative elements.'))\n",
    "  if sum(wgts) <= 0:\n",
    "    raise(WeightSumNotPositive('wgt_quantiles: Sum of <wgts> is not positive.'))\n",
    "    \n",
    "  ## Need to reshape these arrays in order to do broadcasting, so first copy them.\n",
    "  vs  = vss.copy()\n",
    "  qs  = qss.copy()\n",
    "  ws  = wgts.copy()\n",
    "  \n",
    "  ## Sort the vs array and the associated weights.\n",
    "  ## Turn the weights into proper weights and create a cummulative weight array.\n",
    "  idx = np.argsort(vs)\n",
    "  vs  = vs[idx]\n",
    "  ws  = ws[idx]\n",
    "  ws  = ws / np.sum(ws)\n",
    "  cws = np.cumsum(ws)\n",
    "  \n",
    "  N   = np.size(cws)\n",
    "  M   = np.size(qs)\n",
    "  \n",
    "  ## Reshape to broadcast.\n",
    "  cws.shape = (N, 1)\n",
    "  qs.shape  = (1, M)\n",
    "  \n",
    "  ## Use broadcasting to get all comparisons of <cws> with each entry from <qs>.  \n",
    "  ## Do a diff (be mindfull of beginning and end of cws array) to get where the max of vs <= (each element of <qs>).\n",
    "  A   = np.concatenate([np.ones(M).reshape(1,M), (cws <= qs) * 1, np.zeros(M).reshape(1,M)], axis=0)\n",
    "  A   = np.diff(A, axis=0).astype(int)\n",
    "  idx = np.maximum(0, np.where(A == -1)[0] - 1)\n",
    "  \n",
    "  ## Return the weighted quantile value of <vs> against each <qs>.\n",
    "  if scalar_quants:\n",
    "    return(vs[idx][0])\n",
    "  return(vs[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12dca884-c164-40be-8918-11970f52e903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessment_agr_rets(df, ret_field, wgt_field, filt=True):\n",
    "  '''\n",
    "    Computes the weighted averge returns over the date range of the data frame, <df>.\n",
    "    \n",
    "  :param df: A Pandas DataFrame with required fields.\n",
    "           :type pandas.DataFrame.core\n",
    "  :param ret_field: A field representing an numpy array of returns.\n",
    "           :type str\n",
    "  :param wgt_field: A weight field, used to weight the returns in a given row.\n",
    "           :type str\n",
    "           \n",
    "  :returns An numpy array of aggregated returns.\n",
    "  :rtype numpy(np) array(float)\n",
    "  \n",
    "  :packages numpy(np), pandas\n",
    "  '''\n",
    "  ## If the filter value is a scalar -- replicate it to the length of the dataframe, <df>.\n",
    "  fl = filt\n",
    "  if np.shape(filt) == ():\n",
    "    fl = np.repeat(filt, df.shape[0])\n",
    "\n",
    "  return(np.average(np.stack(df.loc[fl, ret_field]), weights=df.loc[fl, wgt_field], axis=0)).astype(float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8963c874-b2bd-4825-aeb8-b05ad9685ef2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## This function does not currently work. There is a mismatch witht he qgt_quantiles function it calls.\n",
    "def assessment_wgt_quant_rets(df, ret_field, wgt_field, quants, filt=True):\n",
    "  '''\n",
    "    Computes weighted quantiles of returns over the date range of the data frame, <df>.\n",
    "    \n",
    "  :param df: A Pandas DataFrame with required fields.\n",
    "            :type DataFrame\n",
    "  :param ret_field: A field representing an numpy array of returns.\n",
    "            :type str\n",
    "  :param wgt_field: A weight field, used to weight the returns in a given row.\n",
    "            :type str\n",
    "  :param quants: A scalar or numpy(np) array of quantiles.\n",
    "           \n",
    "  :returns An numpy array of weighted quantile returns.\n",
    "            :rtype numpy array(float)\n",
    "  '''\n",
    "  ## If the filter value is a scalar -- replicate it to the length of the dataframe, <df>.\n",
    "  fl = filt\n",
    "  if np.shape(filt) == ():\n",
    "    fl = np.repeat(filt, df.shape[0])\n",
    "  \n",
    "  return(wgt_quantiles(np.stack(df.loc[fl, ret_field]), df.loc[fl, wgt_field].to_numpy(), quants))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61c6f5fc-b0de-43d1-81a5-58eb2bdd7c6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def assessment_wgt_median_rets(df, ret_field, wgt_field, filt=True):\n",
    "  '''\n",
    "    Computes weighted quantiles of returns over the date range of the data frame, <df>.\n",
    "    \n",
    "  :param df: A Pandas DataFrame with required fields.\n",
    "            :type DataFrame\n",
    "  :param ret_field: A field representing an numpy array of returns.\n",
    "            :type str\n",
    "  :param wgt_field: A weight field, used to weight the returns in a given row.\n",
    "            :type str\n",
    "           \n",
    "  :returns An numpy array of weighted quantile returns.\n",
    "            :rtype numpy array(float)\n",
    "  '''\n",
    "  ## If the filter value is a scalar -- replicate it to the length of the dataframe, <df>.\n",
    "  fl = filt\n",
    "  if np.shape(filt) == ():\n",
    "    fl = np.repeat(filt, df.shape[0])\n",
    "  \n",
    "  return(wgt_quantiles(np.stack(df.loc[fl, ret_field]), df.loc[fl, wgt_field].to_numpy(), np.array([0.5]))[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd65802-690f-4400-82e9-42b907322a9f",
   "metadata": {},
   "source": [
    "### Data Examination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f297fba-f92c-47ba-9424-f00d4db42222",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e7c858d-66ac-4394-a19f-32e7ebb818c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.PARCEL_TYPE.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "006193a7-ce1b-47d1-b3a9-d7310f8336f8",
   "metadata": {},
   "source": [
    "### Data Cleaning\n",
    "We use regular expression matching to determine which data should be filtered out.\n",
    "We create a dictionary below, badLinesDct, that contains the number of bad lines\n",
    "for each field of interest: FULL_MKT_VALUE (market value of parcel), ACCT (parcel account id), LUC (Land Use Code), ACCR (parcel acreage size)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7d9cddc-2701-4e28-b403-602f3989e08f",
   "metadata": {},
   "outputs": [],
   "source": [
    "badLinesDct = {}\n",
    "good_mkt_filter = df[\"FULL_MKT_VALUE\"].astype(str).str.match(\"^\\d+$\")\n",
    "badLinesDct['FULL_MKT_VALUE']  = df.loc[~ good_mkt_filter].shape[0]\n",
    "\n",
    "good_acct_filter = df[\"ACCT\"].astype(str).str.match(\"^\\d+$\")\n",
    "badLinesDct['ACCT'] = df.loc[~ good_acct_filter].shape[0]\n",
    "\n",
    "good_luc_filter = df['LUC'].astype(str).str.match(\"^\\d+$\")\n",
    "badLinesDct['LUC'] = df.loc[~ good_luc_filter].shape[0]\n",
    "\n",
    "good_accr_filter = df['ACCR'].astype(str).str.match(\"(^(\\d*)\\.\\d+$)|(^\\d+(\\.\\d*)?$)\")\n",
    "badLinesDct['ACCR'] = df.loc[~ good_accr_filter].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dfb63c4-0612-4047-bf0a-b45e74d75456",
   "metadata": {},
   "outputs": [],
   "source": [
    "badLinesDct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccae13e6-7e73-41a9-835e-4d6004f59003",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[~(good_mkt_filter & good_luc_filter & good_acct_filter)].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fb65df-6f9a-47ee-841f-d0a27ab454c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Filter out the bad mkt value, luc, and acct data.\n",
    "df_filt = df[(good_mkt_filter & good_luc_filter & good_acct_filter)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec595b6-14c2-41dc-b89b-06b5bf725c64",
   "metadata": {},
   "outputs": [],
   "source": [
    "dd = df_filt.groupby('ACCT')['FULL_MKT_VALUE'].agg(lambda x: x.size).reset_index(name='MKT_COUNT')\n",
    "\n",
    "dd1 = df_filt.groupby('ACCT')['FULL_MKT_VALUE'].agg(lambda x: any(x == 0)).reset_index(name='MKT_ZERO')\n",
    "\n",
    "ddd = dd.merge(dd1, on='ACCT', how='inner')\n",
    "\n",
    "## Now get the accounts that extend over the 11 period that we have data mkt value data for AND which aren't zero.\n",
    "accts = ddd.loc[(ddd.MKT_COUNT == 11) & (~ ddd.MKT_ZERO)].ACCT\n",
    "\n",
    "## Only use these accounts from the filtered data. This is the data set we will use for analysis.\n",
    "df_clean = df_filt.loc[df_filt['ACCT'].isin(accts), :]\n",
    "df_clean.to_csv(\"clean_tax_ass.psv\", sep='^', encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7495f505-5f17-494c-b6a0-e7d42f0a375a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Describe the reduction in data after cleaning.\n",
    "print(f\"Data cleaning reduced the overall data set by {100.0 * np.round( (df.shape[0] - df_clean.shape[0]) / df.shape[0], 2)}%.\")\n",
    "raw_residencial_count = df.loc[df.LUC == 210].shape[0]\n",
    "filtered_residencial_count = df_clean.loc[df_clean.LUC == 210].shape[0]\n",
    "print(f\"Data cleaning reduced the residencial data set by {100.0 * np.round( (raw_residencial_count - filtered_residencial_count) / raw_residencial_count, 2)}%.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80837409-a7c4-41ee-8558-c305785ff04a",
   "metadata": {},
   "source": [
    "### Compute Market Return Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "540f47e5-aed2-40e7-bd8e-b2fc91154146",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a field, 'mkt_vals' that is an np.array of returns (ordered by YEAR).\n",
    "dd = df_clean.groupby('ACCT').apply(lambda row: np.array(row['FULL_MKT_VALUE'])).reset_index(name='mkt_vals')\n",
    "\n",
    "dd['mkt_rets'] = dd['mkt_vals'].apply(lambda x: np.diff(x)) / dd['mkt_vals'].apply(lambda x: x[:-1])\n",
    "\n",
    "dd['avg_mkt_val'] = dd.apply(lambda row: np.mean(row['mkt_vals']), axis=1)\n",
    "\n",
    "df_rets = df_clean.merge(dd, on='ACCT', how='inner')\n",
    "\n",
    "df_rets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e71a1ab4-2b2d-40f4-9dfd-a84c84240e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "dff = pd.value_counts(df_rets.LUC).to_frame(name='LUC_cnt').reset_index()\n",
    "dff['log_LUC'] = np.log10(dff.LUC_cnt)\n",
    "ax = dff.plot.scatter(x = 'LUC', y='log_LUC', xlabel='LUC\\n(Residencial LUC=210)', ylabel='Log10 of LUC Count', title=\"Log10 of LUC counts\")\n",
    "ax.axvline(210, linestyle='--');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1754bbb7-4dbb-494e-9657-b0375ddbe501",
   "metadata": {},
   "source": [
    "### Aggregated Market Returns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e1b03c-88b7-4b66-b80a-f15b8e780671",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Empty data frame to store returns.\n",
    "df_results = pd.DataFrame({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf1684-ebdd-4581-99a7-78ffdbdfb3eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall Aggregated Market Returns\n",
    "df_results['overall'] = df_rets['mkt_rets'].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c353cea-35db-4613-87cc-4cee74d4834d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregated Market Returns for LUC 210 -- Single family residence.\n",
    "df_results['residence'] = df_rets.loc[df_rets['LUC'] == 210, 'mkt_rets'].agg(np.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3dd2035-2477-4bba-b036-81313074eed4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d14e601f-916b-40f2-9d68-489d71d9824f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute overall weighted returns using the average market value as the weight \n",
    "## -- Also recompute but restrict analysis to single family residences -- LUC = 210.\n",
    "df_results['overall_mkt_wgt'] = assessment_agr_rets(df_rets, 'mkt_rets', 'avg_mkt_val')\n",
    "df_results['residence_mkt_wgt'] = assessment_agr_rets(df_rets, 'mkt_rets', 'avg_mkt_val', filt = df_rets['LUC'] == 210)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89046aa2-c060-447c-99c9-c5193b8b1f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_results.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b82b8e4-f06a-4578-a2c8-0365fae5e9c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_results[['residence', 'overall_mkt_wgt', 'residence_mkt_wgt']].plot( \n",
    "                     xlabel=\"Year\"                     , \n",
    "                     ylabel=\"Assessment Change from Previous Year\",\n",
    "                     title=\"Tax Assessment Comparison (Greenburgh)\" ,\n",
    "                     xticks=df_results.index, rot=90    )\n",
    "ax.set_xticklabels(['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022']);\n",
    "ax.legend(['Residential Avg', 'Overall Mkt Wgt Avg', 'Residential Mkt Wgt Avg']);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60afd725-cece-4feb-b1e0-db066385365c",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Overall Aggregated Market Returns\n",
    "df_results['overall_cs'] = np.cumprod(1.0 + df_rets['mkt_rets'].agg(np.mean)) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f37647e-6eaa-487e-8f90-d4de4797c0c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Aggregated Market Returns for LUC 210 -- Single family residence.\n",
    "df_results['residence_cs'] = np.cumprod(1.0 + df_rets.loc[df_rets['LUC'] == 210, 'mkt_rets'].agg(np.mean)) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ae313-8df4-4e10-8666-441be31608e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_rets.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "220ac287-6fdf-43d4-891f-1898611b5550",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Compute overall weighted returns using the average market value as the weight.\n",
    "df_results['overall_mkt_wgt_cs'] = np.cumprod(1.0 + assessment_agr_rets(df_rets, 'mkt_rets', 'avg_mkt_val')) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22201fc1-b3b8-4272-bb10-eb7c93c59b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Compute overall weighted returns using the average market value as the weight \n",
    "## -- but restrict analysis to single family residences -- LUC = 210.\n",
    "df_results['residence_mkt_wgt_cs'] = np.cumprod(1.0 + assessment_agr_rets(df_rets, 'mkt_rets', 'avg_mkt_val', filt = df_rets['LUC'] == 210)) - 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03b61f5e-5b7b-4e26-87d9-9ba395e10c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = df_results[['residence_cs', 'overall_mkt_wgt_cs', 'residence_mkt_wgt_cs']].plot( \n",
    "                     xlabel=\"Year\"                     , \n",
    "                     ylabel=\"Assessment Change from 2012\",\n",
    "                     title=\"Tax Assessment Comparison (Greenburgh)\\n(Cumulative Change)\" ,\n",
    "                     xticks=df_results.index, rot=90    )\n",
    "ax.set_xticklabels(['2013', '2014', '2015', '2016', '2017', '2018', '2019', '2020', '2021', '2022']);\n",
    "ax.legend(['Residential Avg', 'Overall Mkt Wgt Avg', 'Residential Mkt Wgt Avg']);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
